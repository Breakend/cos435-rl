% Bibliography for Reinforcement Learning from Scratch

% ========== Textbooks and Surveys ==========

@book{sutton2018reinforcement,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  title = {Reinforcement Learning: An Introduction},
  edition = {Second},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  year = {2018},
  isbn = {0262039249}
}

@article{kaelbling1996reinforcement,
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  title = {Reinforcement Learning: A Survey},
  journal = {Journal of Artificial Intelligence Research},
  volume = {4},
  pages = {237--285},
  year = {1996},
  doi = {10.1613/jair.301}
}

% ========== Foundational Works ==========

@article{bellman1957markovian,
  author = {Bellman, Richard},
  title = {A {Markovian} Decision Process},
  journal = {Journal of Mathematics and Mechanics},
  volume = {6},
  number = {5},
  pages = {679--684},
  year = {1957},
  doi = {10.1512/iumj.1957.6.56038}
}

@book{bellman1957dynamic,
  author = {Bellman, Richard},
  title = {Dynamic Programming},
  publisher = {Princeton University Press},
  address = {Princeton, NJ},
  year = {1957}
}

@book{howard1960dynamic,
  author = {Howard, Ronald A.},
  title = {Dynamic Programming and {Markov} Processes},
  publisher = {The Technology Press of MIT and John Wiley \& Sons},
  address = {New York},
  year = {1960}
}

@phdthesis{watkins1989learning,
  author = {Watkins, Christopher John Cornish Hellaby},
  title = {Learning from Delayed Rewards},
  school = {King's College, Cambridge},
  year = {1989}
}

@article{watkins1992q,
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  title = {Q-learning},
  journal = {Machine Learning},
  volume = {8},
  number = {3--4},
  pages = {279--292},
  year = {1992},
  doi = {10.1007/BF00992698}
}

@article{samuel1959studies,
  author = {Samuel, Arthur L.},
  title = {Some Studies in Machine Learning Using the Game of Checkers},
  journal = {IBM Journal of Research and Development},
  volume = {3},
  number = {3},
  pages = {210--229},
  year = {1959},
  doi = {10.1147/rd.33.0210}
}

% ========== Neuroscience ==========

@article{schultz1997neural,
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
  title = {A Neural Substrate of Prediction and Reward},
  journal = {Science},
  volume = {275},
  number = {5306},
  pages = {1593--1599},
  year = {1997},
  doi = {10.1126/science.275.5306.1593}
}

% ========== Deep RL ==========

@article{mnih2015human,
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  title = {Human-level Control through Deep Reinforcement Learning},
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  year = {2015},
  doi = {10.1038/nature14236}
}

@inproceedings{mnih2016asynchronous,
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  title = {Asynchronous Methods for Deep Reinforcement Learning},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages = {1928--1937},
  year = {2016}
}

@article{silver2016mastering,
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  year = {2016},
  doi = {10.1038/nature16961}
}

@article{silver2017mastering,
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  title = {Mastering the Game of {Go} without Human Knowledge},
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  year = {2017},
  doi = {10.1038/nature24270}
}

@article{silver2018general,
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  title = {A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and {Go} through Self-Play},
  journal = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  year = {2018},
  doi = {10.1126/science.aar6404}
}

% ========== Policy Gradient Methods ==========

@article{williams1992simple,
  author = {Williams, Ronald J.},
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  journal = {Machine Learning},
  volume = {8},
  number = {3--4},
  pages = {229--256},
  year = {1992},
  doi = {10.1007/BF00992696}
}

@inproceedings{schulman2015trpo,
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  title = {Trust Region Policy Optimization},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages = {1889--1897},
  year = {2015}
}

@article{schulman2017proximal,
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  title = {Proximal Policy Optimization Algorithms},
  journal = {arXiv preprint arXiv:1707.06347},
  year = {2017}
}

% ========== RLHF and LLMs ==========

@article{ouyang2022training,
  author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
  title = {Training Language Models to Follow Instructions with Human Feedback},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {27730--27744},
  year = {2022}
}

@article{christiano2017deep,
  author = {Christiano, Paul F. and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  title = {Deep Reinforcement Learning from Human Preferences},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  year = {2017}
}

% ========== Classic Psychology ==========

@book{thorndike1911animal,
  author = {Thorndike, Edward L.},
  title = {Animal Intelligence: Experimental Studies},
  publisher = {Macmillan},
  address = {New York},
  year = {1911}
}

@book{skinner1938behavior,
  author = {Skinner, B. F.},
  title = {The Behavior of Organisms: An Experimental Analysis},
  publisher = {Appleton-Century},
  address = {New York},
  year = {1938}
}

@misc{vanroy2024foundations,
  author = {Van Roy, Benjamin},
  title = {Foundations for Reinforcement Learning},
  howpublished = {Lecture Notes, {MS\&E} 338: Aligning Superintelligence, Stanford University},
  year = {2024},
  url = {https://web.stanford.edu/class/msande338/notes/mse338_00_foundations_rl.pdf}
}

@article{lu2023bitbybit,
  author = {Lu, Xiuyuan and Van Roy, Benjamin and Dwaracherla, Vikranth and Ibrahimi, Morteza and Osband, Ian and Wen, Zheng},
  title = {Reinforcement Learning, Bit by Bit},
  journal = {Foundations and Trends in Machine Learning},
  volume = {16},
  number = {6},
  pages = {733--865},
  year = {2023},
  doi = {10.1561/2200000097}
}

% ========== RL Theory ==========

@misc{szepesvari2024rltheory,
  author = {Szepesv{\'a}ri, Csaba},
  title = {Theoretical Foundations of Reinforcement Learning: Lecture Notes},
  howpublished = {University of Alberta, CMPUT 653},
  year = {2024},
  url = {https://rltheory.github.io/}
}

% ========== Reward Hypothesis ==========

@misc{silver2015lectures,
  author = {Silver, David},
  title = {Lectures on Reinforcement Learning},
  howpublished = {University College London},
  year = {2015},
  url = {https://www.davidsilver.uk/teaching/}
}

@article{silver2021reward,
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  title = {Reward is Enough},
  journal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  year = {2021},
  doi = {10.1016/j.artint.2021.103535}
}

% ========== Behavioral Economics and Psychology ==========

@article{ainslie1975specious,
  author = {Ainslie, George},
  title = {Specious Reward: A Behavioral Theory of Impulsiveness and Impulse Control},
  journal = {Psychological Bulletin},
  volume = {82},
  number = {4},
  pages = {463--496},
  year = {1975},
  doi = {10.1037/h0076860}
}

@article{laibson1997golden,
  author = {Laibson, David},
  title = {Golden Eggs and Hyperbolic Discounting},
  journal = {The Quarterly Journal of Economics},
  volume = {112},
  number = {2},
  pages = {443--478},
  year = {1997}
}

% ========== MDP Complexity ==========

@article{ye2011simplex,
  author = {Ye, Yinyu},
  title = {The Simplex and Policy-Iteration Methods Are Strongly Polynomial for the {Markov} Decision Problem with a Fixed Discount Rate},
  journal = {Mathematics of Operations Research},
  volume = {36},
  number = {4},
  pages = {593--603},
  year = {2011}
}

@article{scherrer2016improved,
  author = {Scherrer, Bruno},
  title = {Improved and Generalized Upper Bounds on the Complexity of Policy Iteration},
  journal = {Mathematics of Operations Research},
  volume = {41},
  number = {3},
  pages = {758--774},
  year = {2016}
}

@article{feinberg2014modified,
  author = {Feinberg, Eugene A. and Huang, Jefferson and Scherrer, Bruno},
  title = {Modified Policy Iteration Algorithms Are Not Strongly Polynomial for Discounted Dynamic Programming},
  journal = {Operations Research Letters},
  volume = {42},
  number = {6--7},
  pages = {429--431},
  year = {2014}
}

% ========== Scalable RL ==========

@inproceedings{espeholt2018impala,
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymyr and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  title = {{IMPALA}: Scalable Distributed Deep-{RL} with Importance Weighted Actor-Learner Architectures},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages = {1407--1416},
  year = {2018}
}

@inproceedings{horgan2018distributed,
  author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and van Hasselt, Hado and Silver, David},
  title = {Distributed Prioritized Experience Replay},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2018}
}

@inproceedings{kapturowski2019recurrent,
  author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  title = {Recurrent Experience Replay in Distributed Reinforcement Learning},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2019}
}

@article{piche2025pipelinerl,
  author = {Pich{\'e}, Alexandre and Kamalloo, Ehsan and Pardinas, Rafael and Chen, Xiaoyin and Bahdanau, Dzmitry},
  title = {{PipelineRL}: Faster On-policy Reinforcement Learning for Long Sequence Generation},
  journal = {arXiv preprint arXiv:2509.19128},
  year = {2025}
}

@article{khatri2025scalerl,
  author = {Khatri, Devvrit and Madaan, Lovish and Tiwari, Rishabh and Bansal, Rachit and Duvvuri, Sai Surya and Zaheer, Manzil and Dhillon, Inderjit S. and Brandfonbrener, David and Agarwal, Rishabh},
  title = {The Art of Scaling Reinforcement Learning Compute for {LLMs}},
  journal = {arXiv preprint arXiv:2510.13786},
  year = {2025}
}

% ========== AI Safety and Reward Hacking ==========

@techreport{openai2024o1systemcard,
  author = {{OpenAI}},
  title = {{OpenAI} o1 System Card},
  institution = {OpenAI},
  year = {2024},
  month = {December},
  url = {https://cdn.openai.com/o1-system-card-20241205.pdf}
}

@misc{openai2016faulty,
  author = {Clark, Jack and Amodei, Dario},
  title = {Faulty Reward Functions in the Wild},
  howpublished = {OpenAI Blog},
  year = {2016},
  month = {December},
  url = {https://openai.com/index/faulty-reward-functions/}
}

@article{krakovna2020specification,
  author = {Krakovna, Victoria and Uesato, Jonathan and Mikulik, Vladimir and Rahtz, Matthew and Everitt, Tom and Kumar, Ramana and Kenton, Zac and Leike, Jan and Legg, Shane},
  title = {Specification Gaming: The Flip Side of {AI} Ingenuity},
  journal = {DeepMind Blog},
  year = {2020},
  url = {https://deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity}
}
